<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alvaro Gamundi - Portfolio</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <style>


	.dataset-link {
    		color: #007BFF;
    		font-weight: bold;
    		text-decoration: none;
    		font-size: 1rem;
    		padding: 0; /* sin borde ni relleno */
    		border: none;
    		background: none;
	}

	.dataset-link:hover {
    		text-decoration: underline;
    		color: #0056b3;
	}



        body {
            background-color: #f1f1f1;
            color: black;
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
        }

        h1 {
            text-align: left;
            padding: 20px;
            color: black;
            font-weight: 700;
            font-size: 4rem;
        }

        .content {
            padding: 50px 20px;
        }

        .typing-container {
            display: inline-block;
            border-right: 2px solid black;
            white-space: pre-wrap;
            overflow: hidden;
            font-size: 2rem;
            animation: blink 0.7s step-end infinite alternate;
        }

        @keyframes blink {
            from { border-right-color: black; }
            to { border-right-color: transparent; }
        }

        .projects {
            margin-top: 80px;
        }

        .project-column {
            width: 100%;
            margin-bottom: 100px; /* M√°s espacio entre proyectos */
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .project-column img {
            width: 50%;
            height: 300px;
            object-fit: cover;
            border-radius: 8px;
            margin-bottom: 15px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .project-column a:not(.dataset-link) {
    		font-size: 1.5rem;
    		font-weight: bold;
    		color: black;
    		text-decoration: none;
    		display: inline-block;
    		padding: 8px 12px;
    		border: 2px solid transparent;
    		border-radius: 5px;
    		transition: all 0.3s ease;
	}


        .project-column a:hover {
            background-color: #d6d6d6;
            color: black;
            border: 2px solid #c0c0c0;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);
        }

        .more-info-btn {
            font-size: 1.2rem;
            font-weight: normal;
            color: #007BFF;
            text-decoration: none;
            padding: 10px;
            border: 2px solid #007BFF;
            border-radius: 5px;
            margin-top: 10px;
            display: inline-block;
            transition: all 0.3s ease;
        }

        .more-info-btn:hover {
            background-color: #007BFF;
            color: white;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);
        }

        .more-info-content {
            display: none;
            margin-top: 10px;
            padding: 20px;
            background-color: #f9f9f9;
            border: 2px solid #ccc;
            border-radius: 5px;
            font-size: 1rem;
            color: #333;
            text-align: left;
            width: 90%; /* M√°s ancho: 90% del ancho de la p√°gina */
            max-width: 1200px; /* Ancho m√°ximo */
        }

        .more-info-content h3 {
            font-size: 1.5rem;
            margin-bottom: 10px;
        }

	.toggle-section {
    		cursor: pointer;
    		font-weight: bold;
    		color: #333;
    		padding: 5px 0;
    		font-size: 1.3rem; /* Larger font size for the headings */
        }

        .toggle-section:hover {
            color: #007BFF;
        }

        .nested-content {
    		margin-left: 20px;
		margin-right: 40px;
    		display: none;
    		text-align: justify;  /* A√±adido: justifica el texto */
        }
    </style>
</head>
<body>
    <div class="content">
        <h1>Hello, welcome to my portfolio!</h1>
        <div class="typing-container" id="typing-text"></div>

        <div class="projects">

	    <div class="project-column">
                <img src="flight.jpg" alt="Flight Experience Feedback Project">
                <a href="https://frontend-app-flights.vercel.app///" target="_blank">FLIGHT EXPERIENCE FEEDBACK ANALYZER</a>
                <a href="javascript:void(0);" class="more-info-btn" id="more-info-btn-4" onclick="toggleMoreInfo(4)">More Info</a>
                <div class="more-info-content" id="more-info-4">
                    <h3>Table of Contents:</h3>
                    <ul>
			<li class="toggle-section" onclick="toggleNested(1, 1)">1. üìå Introduction</li>
			<div class="nested-content" id="nested-1-1">
    			   <p>This project aims to build an intelligent system capable of <strong>predicting the sentiment of user comments about flight experiences</strong>. For this purpose, a dataset from Kaggle was used, containing <strong>real tweets from passengers</strong> about flights with different airlines.</p>
			   <p>The ultimate goal is to develop a <strong>production-ready web application</strong> where users can share their opinions about their flights, and the system, using natural language processing (NLP) models, automatically classifies the comments as <strong>positive</strong>, <strong>negative</strong>, or <strong>neutral</strong>.</p>
			   <p>This way, airlines can use the collected information to:</p>
			   <ul>
        			<li>Monitor customer satisfaction in real-time.</li>
        			<li>Identify areas for service improvement.</li>
        			<li>Make data-driven decisions.</li>
    			   </ul>
			   <p></p>
			</div>


			<li class="toggle-section" onclick="toggleNested(1, 2)">2. üìä Dataset</li>
			<div class="nested-content" id="nested-1-2">
			   <p>The dataset was extracted from Kaggle and can be found at the following link:<br><a href="https://www.kaggle.com/code/serkanp/sentiment-analysis-of-airline-tweets-and-comments" class="dataset-link" target="_blank">Sentiment Analysis of Airline Tweets and Comments</a></p>
    			   
			   <p>It is a CSV file containing <strong>15 columns</strong> and <strong>14,640 rows</strong> of tweets from users sharing their flight experiences.</p>
			   <p>The key variables used in this project are:</p>
			   <ul>
        			<li><strong>text:</strong> The tweet text published by the user about their flight experience with a specific airline.</li>
        			<li><strong>airline_sentiment:</strong> The target variable, which classifies the sentiment of the experience into <strong>positive</strong>, <strong>negative</strong>, or <strong>neutral</strong>.</li>
        		   </ul>
			   <p>These two variables are the core of the sentiment analysis modeling.</p>
			   <p></p>
			</div>


			<li class="toggle-section" onclick="toggleNested(1, 3)">3. üß™ Model Development</li>
			<div class="nested-content" id="nested-1-3">
    			   <p>In this stage, I focused on obtaining the best possible model to deploy in a production environment. To achieve this, I conducted an exploratory data analysis that helped me understand the behavior of the text and evaluate whether it was feasible to use more traditional probabilistic models, such as <strong>Naive Bayes</strong>.</p>
			   <p>I also performed thorough text preprocessing, selected appropriate evaluation metrics, and tested various models, comparing their performance using <strong>fine-tuning techniques</strong>. This process allowed me to select the most robust model in terms of <strong>accuracy</strong> and <strong>generalization</strong>.</p>
    			 </div>

			<li class="toggle-section" onclick="toggleNested(1, 4)">4. üìä EDA (Exploratory Data Analysis)</li>
			<div class="nested-content" id="nested-1-4">
    			   <p>In this section, I performed an exploratory analysis of the <strong>text</strong> variable in relation to the <strong>airline_sentiment</strong> label.</p>
    			   <p>The goal was to identify <strong>statistical patterns</strong> that help understand how the textual data is structured depending on the sentiment (positive, neutral, or negative).</p>
			   <p>Some of the key analyses included:</p>
			   <ul>
        			<li><strong>Number of words per tweet</strong></li>
        			<li><strong>Number of unique words per tweet</strong></li>
				<li><strong>Average word length per tweet</strong></li>
        			<li><strong>Total number of characters per tweet</strong></li>
				<li><strong>Class balance across sentiment categories</strong></li>
        		   </ul>
			   <p>This EDA step was essential to assess the </strong>feasibility of simpler models</strong> and guided decisions in <strong>text cleaning</strong>, <strong>tokenization</strong>, and <strong>feature selection</strong>.</p>
			   <p></p>

			
			<p><strong>‚öôÔ∏è üß† Insights from Exploratory Analysis</strong></p>
			
    			   <p>From the exploratory analysis of the text data, several key insights emerge:</p>
    			   <ul>
    				<li>
        				<strong>Tweet Length (in words):</strong><br>Negative tweets tend to be longer on average, peaking around 22‚Äì25 words. In contrast, positive and neutral tweets are generally shorter and more dispersed. This suggests that users are more verbose when expressing dissatisfaction.<br>
        				<img src="WPT1.jpg" alt="Words per tweet" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
    				</li>

    				<li>
        				<strong>Number of Unique Words:</strong><br>Negative tweets also exhibit a higher number of unique words per tweet, indicating a richer vocabulary likely used to detail complaints. Positive and neutral tweets show a more modest and uniform distribution of unique terms.<br>
        				<img src="WPT2.jpg" alt="Unique words per tweet" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
    				</li>

    				<li>
        				<strong>Average Word Length:</strong><br>Across all sentiment categories, the average word length remains fairly stable, typically between 4 and 6 characters. This implies that while users may write more when unhappy, the complexity of the vocabulary used does not vary significantly with sentiment.<br>
        				<img src="WPT3.jpg" alt="Average word length" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
    				</li>

   				<li>
        				<strong>Tweet Length (in characters):</strong><br>Similar to word count, tweets labeled as negative often reach the character limit (140‚Äì160 characters), whereas positive and neutral tweets are generally shorter. This reinforces the idea that users elaborate more when describing negative experiences.<br>
        				<img src="WPT4.jpg" alt="Tweet length in characters" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
					<strong>Conclusion:</strong><br>These insights confirm that <strong>tweet length and lexical diversity are strong indicators of sentiment polarity</strong>. Negative feedback is usually more detailed, which can be leveraged in model training by including features related to text length or richness. This justifies exploring more advanced models capable of capturing nuanced language patterns beyond simple keyword-based approaches.
    				</li>

				<p></p>
    				<li>
        				<strong>Class Balance Across Sentiment Categories:</strong><br>The dataset shows a clear imbalance across sentiment categories. The <strong>negative</strong> class dominates with over <strong>8,600 tweets</strong>, while the <strong>neutral</strong> and <strong>positive</strong> classes have approximately <strong>2,900</strong> and <strong>2,100</strong> samples, respectively.
    				</li>
			</ul>
			<p>While this imbalance might suggest the need for resampling techniques such as <strong>oversampling</strong> or <strong>undersampling</strong>, it's important to consider the implications:</p>
			<ul>
				<li>
        				<strong>Undersampling</strong> the majority class (negative) could lead to <strong>information loss</strong>, which is especially harmful when using deep models like <strong>BERT</strong> that benefit from large and diverse datasets.
    				</li>
				<p></p>	
				<li>
        				<strong>Oversampling</strong> the minority classes may introduce <strong>overfitting</strong>, particularly if the same samples are repeated multiple times during training.
					<img src="balance.jpg" alt="Tweet length in characters" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
    				</li>
			</ul>
			<p></p>	
			</div>

			<li class="toggle-section" onclick="toggleNested(1, 6)">6. üßπ Data Cleaning</li>
			<div class="nested-content" id="nested-1-6">
    			   <p>At this stage, the text variable ‚Äî containing the user tweets ‚Äî was preprocessed to remove noise that could negatively affect model performance. Tweets often include a variety of elements that do not contribute to sentiment analysis and can mislead or clutter the learning process.</p>
    			   <p>The following cleaning operations were applied:</p>
			   <ul>
  				<li>üîó <strong>Removing URLs:</strong> Links were stripped from the text since they provide no useful sentiment information and can distort vector representations.</li>
 				<li>üßë‚Äçüíª <strong>Replacing user mentions:</strong> Mentions (e.g., <code>@username</code>) were replaced with a generic placeholder to preserve structure without introducing noise.</li>
  				<li>üòÄ <strong>Removing emojis:</strong> Emojis were removed to simplify the text and maintain consistency across samples.</li>
  				<li>üôÇ <strong>Eliminating emoticons:</strong> Classic emoticons like <code>:)</code> or <code>:(</code> were also removed, as they may not be interpreted reliably by standard tokenizers.</li>
  				<li>#Ô∏è‚É£ <strong>Removing the '#' from hashtags:</strong> Hashtag symbols were deleted while preserving the associated words (e.g., <code>#delayed</code> ‚Üí <code>delayed</code>) to retain semantic meaning.</li>
  				<li>‚ê£ <strong>Normalizing whitespace:</strong> Extra spaces were removed to avoid tokenization issues.</li>
			   </ul>
			   <p>These preprocessing steps were crucial to creating a <strong>clean and standardized corpus</strong>, which improves the <strong>quality of tokenization and vectorization</strong>, and ultimately enhances the model‚Äôs accuracy and robustness.</p>	
			<p></p>	
			</div>

			<li class="toggle-section" onclick="toggleNested(1, 7)">7. üîÑ Variable Transformation</li>
			<div class="nested-content" id="nested-1-7">


  			    <p>Before detailing the specific transformations applied, it's important to note that multiple models were tested prior to selecting the final one. These included:</p>
  				<ul>
    					<li><strong>Naive Bayes</strong></li>
    					<li><strong>Simple feedforward neural networks</strong></li>
    					<li><strong>Recurrent neural networks (RNNs)</strong></li>
  				</ul>
  			    <p>Each of these models required input features in <strong>different formats</strong>, so the text data needed to be transformed accordingly.</p>

  				<ul>
    					<li>
        				<strong>üìà TF-IDF (Term Frequency ‚Äì Inverse Document Frequency):</strong><br>Used as input for both Naive Bayes and deep feedforward neural networks. This method converts text into sparse numeric vectors by capturing the importance of each word in a document relative to the entire corpus.
					</li>

    					<li>
        				<strong>üî§ Word Embeddings with Word2Vec:</strong><br>Applied in the case of recurrent neural networks (RNNs), where each word was transformed into a dense, fixed-size vector. These embeddings capture semantic relationships between words and were either pretrained or trained on the dataset itself.<br>
    					</li>
				</ul>

    			   <p>Finally, the model selected for deployment was based on Transformers, specifically using BERT with fine-tuning of the last layers (this process will be explained in more detail in a later section).</p>
    			   <p>To feed the text data into BERT, the input was tokenized into tensors using a pre-trained BERT tokenizer. This process generated two key components:</p>
				<ul>
    					<li>
        				<strong>input_ids: </strong>the encoded representation of each token (word/subword) in the input text.
					</li>

    					<li>
        				<strong>attention_mask </strong>a binary mask that indicates which tokens should be attended to (1) and which are padding (0). This helps BERT ignore padded tokens during computation.
    					</li>
				</ul>
			<p>On the other hand, the target variable (airline_sentiment), originally a column containing the string labels <strong>"positive"</strong>, <strong>"neutral"</strong>, and <strong>"negative"</strong>, was converted into a numeric tensor with values:</p>
        			<ul>
    					<li>0 ‚Üí positive</li>
    					<li>1 ‚Üí neutral</li>
    					<li>2 ‚Üí negative</li>
  				</ul>
  			<p>This transformation allowed the model to perform <strong>multi-class classification</strong> efficiently using a categorical loss function like <strong>CrossEntropyLoss</strong>.</p>
			</div>

			<li class="toggle-section" onclick="toggleNested(1, 8)">8. üß† Model Training with BERT</li>
			<div class="nested-content" id="nested-1-8">
    			   <p>To properly assess model performance and select the best architecture, the dataset was first split into training, validation, and test sets:</p>
				<ul>
    					<li>
        				The model was trained on the training set.
					</li>
    					<li>
        				It was evaluated on the validation set after each epoch.
    					</li>	
					<li>
        				Finally, it was tested on unseen data to assess its real-world performance.
    					</li>
				</ul>
    			   <p>The final model used was:</p>
			   <p>from transformers import BertForSequenceClassification<br>model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
			   <p></p>	
			<p><strong>‚öôÔ∏è Optimization and Metrics</strong></p> 
			<p>To train and evaluate the model effectively, the following configuration was used:</p> 
				<ul>
    					<li>
        				<strong>Optimizer</strong>: AdamW, a variant of the Adam optimizer tailored for Transformers, with a learning rate of 3e-5.
					</li>
    					<li>
        				<strong>Loss Function</strong>: CrossEntropyLoss, well-suited for multi-class classification.
    					</li>	
					<li>
        				<strong>Evaluation Metrics</strong>:
						<ul>
        						<li>Weighted Precision</li>
        						<li>Weighted Recall</li>
      						</ul>
    					</li>
				</ul>
			  <p>These metrics were calculated using <code>torchmetrics</code>, taking class imbalance into account for both training and validation phases.</p>
			  <p></p>	
			  <p><strong> üèÜ Best Model Selection Strategy</strong></p> 
			  <p>To ensure optimal model performance, a validation-based checkpointing approach was used:</p> 
				<ul>
    					<li>
        				After every epoch (and intermittently during training), the model‚Äôs performance on the validation set was evaluated.
					</li>
    					<li>
        				If the validation precision improved, the current model state was saved using:
    					</li>	
				</ul>
			  <p>torch.save(best_model_state, "mejor_modelo_por_precision.pt")</p>
			  <p>This approach ensured that the best model ‚Äî in terms of correctly classifying sentiments ‚Äî was retained and later deployed.</p>
			  <p></p>	
			  </div>

			<li class="toggle-section" onclick="toggleNested(1, 9)">9. üìà Final Results</li>
			<div class="nested-content" id="nested-1-9">
    			   <p>For training, a <strong>GPU provided by Google Colab</strong> was used, which significantly accelerated the fine-tuning of the BERT model.</p>
			  <p>The final model achieved an <strong>F1-score (weighted average) of 0.84</strong> on both the validation and test sets, indicating:
				<ul>
    					<li>
        				<strong>Strong generalization performance</strong>
					</li>
    					<li>
        				<strong>No signs of overfitting</strong>
					</li>
					<li>
        				<strong>~10 percentage point improvement</strong> over other models (Naive Bayes, feedforward and recurrent neural networks)
					</li>
				</ul>
			  <p>This performance gain is likely due to <strong>BERT's capacity to capture contextual information</strong> across entire sentences, allowing it to better understand sentiment nuances.</p>
			  <p>Below are the classification metrics obtained on the <strong>test set</strong>:</p>
			  <img src="score.jpg" alt="Tweet length in characters" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
    				</li>
			  <p></p>			
			  <p><strong> üîç Class-wise Analysis</strong></p> 
				<ul>
    					<li>
        				<strong>Positive (0)</strong>: The model achieves <strong>0.84 precision</strong> and <strong>0.76 recall</strong>, resulting in an <strong>F1-score of 0.80</strong>. This means it correctly identifies many positive tweets, though some are misclassified as neutral or negative. Still, when it predicts "positive", it is usually right.
					</li>
    					<li>
        				<strong>Neutral (1)</strong>: This is the most challenging class for the model, with the lowest <strong>F1-score (0.69)</strong>, <strong>precision (0.71)</strong>, and <strong>recall (0.68)</strong>. This likely stems from the ambiguous nature of neutral language, which often overlaps with both positive and negative expressions.
					</li>
					<li>
        				<strong>Negative (2)</strong>: This is where the model performs best. With a <strong>0.90 F1-score</strong> and <strong>0.92 recall</strong>, it captures nearly all negative tweets, and with very high <strong>precision (0.89)</strong>. This strong performance is expected due to the abundance and clarity of negative expressions in the dataset.
					</li>
				<ul>
			<img src="f1_score_by_class_blue.png" alt="Tweet length in characters" style="display: block; margin: 20px auto; max-width: 90%; width: 700px; height: auto; border-radius: 10px;">
		
		        </div>

			<li class="toggle-section" onclick="toggleNested(1, 10)">10. üöÄ Production Deployment</li>
			<div class="nested-content" id="nested-1-10">
			<p>To deploy the sentiment classification system, a <strong>production-ready architecture</strong> was implemented, allowing users to access the application through <strong>login or registration</strong>.</p>
			

			<p><strong> üîß Backend with FastAPI</strong></p>
			<p>The backend was developed using <strong>FastAPI</strong> and exposes the following main POST endpoints:</p>
				<ul>
    					<li>
        				<strong>/register</strong>: Allows users to create an account
					</li>
    					<li>
        				<strong>/login</strong>: Authenticates users and returns an access token
					</li>
					<li>
        				<strong>/predict</strong>: Receives a text input and returns a sentiment prediction along with a personalized, empathetic message
					</li>
				</ul>
			<p>Example responses based on sentiment:</p>
				<ul>
    					<li>
        				<strong>positive:</strong><br>"We're happy to hear your experience was good! We hope to have you flying with us again soon."<br>
					</li>
    					<li>
        				<strong>neutral:</strong><br>"Thank you for your feedback! We are constantly working to improve our service."<br>
					</li>
					<li>
        				<strong>negative:</strong><br>"We're truly sorry for your experience. We will get in touch with you to improve anything that needs attention."<br>
					</li>
				</ul>
			<p>This approach simulates a <strong>real-world customer service response</strong> tailored to the detected sentiment.</p>
			<p>All endpoints are automatically documented through FastAPI's interactive interface at /docs, making them easy to test and integrate.</p>
			<br>

			<p><strong> üõ†Ô∏è Authentication and Database</strong></p>
			<p>To manage user accounts, a <strong>SQL (PostgreSQL)</strong> database was created using <strong>Supabase</strong>, where the following data is stored:</p>
				<ul>
    					<li>
        				Registered user information
					</li>
    					<li>
        				Encrypted passwords
					</li>
					
				</ul>
			<p>The <strong>backend</strong> handles communication with the database to authenticate users. The <strong>frontend never directly accesses</strong> sensitive data, ensuring greater security.</p>
			<br>

			<p><strong> üîê Security and Encryption</strong></p>
			<p>Password protection is implemented using <strong>bcrypt</strong> encryption, following industry best practices. Additionally:<p>
				<ul>
    					<li>
        				<strong>JWT tokens</strong> are used for authentication after login
					</li>
    					<li>
        				Only authenticated users can access the /predict endpoint
					</li>
					<li>
        				Passwords are never stored in plain text
					</li>	
				</ul>
			<p>pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")<br>oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/login")<p>

			</div>
			

			<li class="toggle-section" onclick="toggleNested(1, 11)">11. ‚òÅÔ∏è Cloud Deployment</li>
			<div class="nested-content" id="nested-1-11">
			<p>To make the application available online and production-ready, a cloud-based deployment pipeline was implemented using <strong>Docker</strong>, <strong>AWS ECR</strong>, and <strong>AWS App Runner</strong>.</p>

			<p><strong> üê≥ Docker Containerization</strong></p>
			<p>The backend application was containerized using <strong> Docker</strong> to ensure portability and consistency across environments. This process included:</p>
				<ul>
    					<li>
        				Writing a Dockerfile that installs all dependencies and launches the FastAPI app
					</li>
    					<li>
        				Copying the <strong>trained model</strong> directly into the Docker image so that predictions can be made without needing to reload the model externally
					</li>
					<li>
        				Avoiding sensitive or unnecessary files using a .dockerignore file
					</li>	
				</ul>
			<p><strong> üî• Dockerfile Highlights</strong></p>
			<p>The Dockerfile ensured:</p>
				<ul>
    					<li>
        				Lightweight image based on python:3.10-slim
					</li>
    					<li>
        				Installation of dependencies via requirements.txt
					</li>
					<li>
        				Inclusion of model files and backend code
					</li>
					<li>
        				Command to launch FastAPI with Uvicorn
					</li>		
				</ul>

			<p><strong> üìÅ .dockerignore</strong></p>
			<p>To keep the image clean and secure, a <code>.dockerignore</code> file was created with the following contents:</p>

			<pre><code>pycache/
			*.pyc
			*.pyo
			.env
			.git
			.vscode
			venv/
			env/
			*.log
			*.sqlite3
			*.db
			data/
			</code></pre>

			<p>This prevents unnecessary files (such as local caches, logs, databases, and development environment folders) from being copied into the Docker image.</p>

			<p>Once the Docker image was built and tested locally, it was pushed to <strong>AWS ECR</strong> (Elastic Container Registry). From there, the backend was deployed using <strong>AWS App Runner</strong>.</p>
		
			<p><strong> üöÄ Deployment with AWS ECR & App Runner</strong></p>
			<p><strong>App Runner</strong> allows applications to be deployed directly from Docker images (hosted in ECR) without the need to manage any servers or infrastructure manually.</p>

			<p>The deployment process involved:</p>

				<ul>
  					<li>Creating an <strong>ECR</strong> repository and pushing the image containing the backend code</li>
  					<li>Setting up an <strong>App Runner</strong> service connected to that repository</li>
  					<li>Configuring the necessary environment variables (such as database URL, secret key, and JWT algorithm)</li>
  					<li>Verifying that the backend was available over <strong>HTTPS</strong> with basic auto-scaling enabled</li>
				</ul>

			<p>This approach made it easy to deploy the backend to the cloud, making it publicly accessible and ready to connect with the frontend.</p>
			</div>

			<li class="toggle-section" onclick="toggleNested(1, 12)">12. üåê Frontend Deployment with Vercel</li>
			<div class="nested-content" id="nested-1-12">
			<p>The frontend of the application was developed using <strong>React</strong> and deployed via <strong>Vercel</strong>, a platform that offers instant hosting for frontend applications with continuous integration.</p>
			<p><strong> üß± Frontend Stack and Structure</strong></p>
			<p>The frontend communicates with the backend via <code>fetch</code> requests using the <code>POST</code> method. Key aspects include:</p>

				<ul>
  					<li>
    						<strong>Token-based authentication:</strong> The JWT token obtained during login is stored in 
    						<code>localStorage</code> and included in the <code>Authorization</code> header when making requests to 
    						<code>/predecir</code>.
  					</li>

  					<li>
    						The following <strong>endpoints</strong> are consumed from the backend hosted on App Runner:
    						<ul>
      							<li><code>/login</code></li>
      							<li><code>/registro</code></li>
      							<li><code>/predecir</code></li>
    						</ul>
  					</li>
				</ul>

			<p>The frontend properly handles authentication errors and model responses, displaying informative messages to the user.</p>
			<p><strong> üöÄ Deployment on Vercel</strong></p>
			<p>The React app was deployed on <strong>Vercel</strong>, which automatically detects changes pushed to GitHub and performs the build and deployment process seamlessly.</p>
			</div>

			<li class="toggle-section" onclick="toggleNested(1, 13)">13. üí° Future Work</li>
			<div class="nested-content" id="nested-1-13">
<p>Although the application is functional and production-ready, there are several areas that could be improved or expanded in future iterations:</p>

<ul>
  <li>
    <strong>Improve performance on the "neutral" class</strong><br>
    The model shows lower precision and recall for neutral tweets. Future work could explore techniques such as:
    <ul>
      <li>Data augmentation for underrepresented classes</li>
      <li>Incorporating sentiment lexicons or external context</li>
    </ul>
  </li>

  <li>
    <strong>Add user feedback collection</strong><br>
    Allow users to rate the accuracy of the prediction and use that feedback to improve model retraining.
  </li>

  <li>
    <strong>Integrate CI/CD pipeline</strong><br>
    Automate testing and deployment for both backend and frontend using tools like GitHub Actions.
  </li>

  <li>
    <strong>Add password recovery and email confirmation</strong><br>
    Improve the authentication flow by adding options for password reset and verifying user emails via Supabase or a third-party service.
  </li>

  <li>
    <strong>Multilingual support</strong><br>
    Extend the model to classify sentiment in other languages, such as Spanish or French, using multilingual models like <code>bert-base-multilingual-cased</code>.
  </li>

  <li>
    <strong>Analytics dashboard for admins</strong><br>
    Create a dashboard where admins (e.g., airline staff) can see aggregated sentiment trends and filter feedback by time, sentiment, or keywords.
  </li>
</ul>

			</div>


		</div>

	    </div>










            <div class="project-column">
                <img src="risk_score.jpg" alt="Bank Loan Risk Project">
                <a href="https://appriskscoring-ehuke5ze63jgdfmnspqoqx.streamlit.app/" target="_blank">BANK LOAN RISK SCORE ANALYZER</a>
                <a href="javascript:void(0);" class="more-info-btn" id="more-info-btn-1" onclick="toggleMoreInfo(1)">More Info</a>
                <div class="more-info-content" id="more-info-1">
                    <h3>Table of Contents:</h3>
                    <ul>
			<li class="toggle-section" onclick="toggleNested(2, 1)">1. Introduction</li>
			<div class="nested-content" id="nested-2-1">    				
				<p>The client is an online platform specializing in offering various types of <strong>loans</strong> to <strong>urban customers</strong>. This platform enables borrowers to easily and quickly access loans with <strong>competitive interest rates</strong> through a <strong>user-friendly interface</strong>.</p>
				<p>When a loan application is received, the company evaluates the applicant's profile to decide whether to approve the loan. Similar to other lending companies, loans granted to <strong>"high-risk"</strong> customers are the primary source of <strong>financial losses</strong>. Therefore, the company aims to identify these <strong>high-risk borrowers</strong> and the potential losses associated with their loans to improve <strong>capital management</strong>, optimize its <strong>loan portfolio</strong>, and enhance its <strong>risk assessment process</strong>.</p>
    			</div>

			<li class="toggle-section" onclick="toggleNested(2, 2)">2. Objectives</li>
			<div class="nested-content" id="nested-2-2">
    				<p>Development of an <strong>advanced analytical tool</strong> that uses predictive machine learning models to estimate the <strong>expected financial loss</strong> of each new <strong>client-loan relationship</strong>.</p>
    			</div>

			<li class="toggle-section" onclick="toggleNested(2, 3)">3. Understanding the Problem</li>
			<div class="nested-content" id="nested-2-3">
    				<p><strong>Credit risk</strong> refers to the possibility that a client will fail to meet their contractual obligations, such as <strong>mortgages</strong>, <strong>credit card debts</strong>, and other types of loans.</p>

    				<p>Minimizing the risk of <strong>default</strong> is one of the primary concerns of financial institutions. For this reason, <strong>commercial banks</strong>, <strong>investment banks</strong>, <strong>venture capital funds</strong>, <strong>asset management companies</strong>, and <strong>insurance companies</strong> are increasingly relying on technology to predict which clients are more likely to default on their debts.</p>

    				<p>The management of credit risk across a portfolio of financial instruments must account for the likelihood of future deterioration and is commonly measured through <strong>expected loss</strong> and <strong>lifetime expected credit loss</strong>. To comply with <strong>IFRS 9</strong> or <strong>CECL</strong>, risk managers must calculate the expected credit loss over the life of the portfolio of financial instruments.</p>

    				<p><strong>Machine Learning models</strong> have helped these organizations improve the accuracy of their <strong>credit risk analysis</strong>, providing a scientific method to identify potential defaulters in advance.</p>
			</div>

                      	<li class="toggle-section" onclick="toggleNested(2, 4)">4. Project design</li>
			<div class="nested-content" id="nested-2-4">
				<p><strong><u>METHODOLOGY</u></strong></p> 
    				<p>To estimate the expected loss (EL) associated with a specific loan application, three key risk parameters are considered:</p>
    				<ol>
        				<li><strong>Probability of Default (PD)</strong>: This represents an internal credit rating assigned to each client, which helps estimate the likelihood of the client defaulting on their obligations.</li>
        				<li><strong>Loss Given Default (LGD)</strong>: A critical metric in risk analysis, it indicates the percentage of exposure that is not expected to be recovered in the event of a default.</li>
        				<li><strong>Exposure at Default (EAD)</strong>: This refers to the percentage of the outstanding debt at the time of default.</li>
    				</ol>
    				<p>To estimate the expected loss for each loan transaction, three predictive machine learning models will be developed to estimate these parameters, and their predictions will be combined to calculate the expected loss for each loan.</p>
				<!-- Image added here -->
				<div style="text-align: center;">
    					<img src="imagen1.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 11.5 / 1;">
				</div>

				<p>Where P is the loan principal (the amount of money the borrower wishes to apply for).</p>

    				<!-- Second image added -->
    				<div style="text-align: center;">
        				<img src="Imagen2.png" alt="Second Project Design Image" style="width: 80%; height: auto; aspect-ratio: 15.5 / 9;">
    				</div>
				
				<p>The model designed to estimate the <strong>probability of default</strong> will utilize a <strong>logistic regression algorithm</strong>. In the context of regulated financial services, "black-box" algorithms are not ideal because their lack of transparency and auditability could introduce macroeconomic risks. Additionally, legal requirements may mandate a certain level of <strong>explainability</strong>. To mitigate these concerns, a transparent AI model, such as logistic regression, will be employed, as it provides clear, understandable reasoning for its decision-making process.</p>

				<p>For the estimation of <strong>exposure at default</strong> and <strong>loss given default</strong>, various combinations of algorithms (including Ridge, Lasso, LightGBM, and others) and their corresponding hyperparameters were evaluated to identify the most effective approach. Ultimately, the <strong>LightGBM algorithm</strong> was selected for both tasks, based on its exceptional performance.</p>
			<p><strong><u>ENTITIES AND DATA</u></strong></p>
			<p>The analyzed data contains information collected by a american company about two main entities:</p>

			<p><strong>Borrowers:</strong> The dataset includes features that capture details about the applicant's profile, such as their employment history, the number of mortgages and credit lines, annual income, and other personal information.</p>

			<p><strong>Loans:</strong> The remaining features provide information about the loan, such as the loan amount, interest rate, loan status (whether the loan is current or delinquent), loan term (either 36 or 60 months), among others.</p>
			


			</div>

			<li class="toggle-section" onclick="toggleNested(2, 5)">5. Data quality</li>
                        <div class="nested-content" id="nested-2-5">
				<p>In this stage of the project, general data quality correction processes have been applied, such as:</p>
    				<ul>
        				<li>Correction of names</li>
        				<li>Elimination of variables with unique values</li>
        				<li>Removal of duplicates</li>
        				<li>Imputation of null values</li>
        				<li>Correction of outliers</li>
    				</ul>
			<br>
			</div>
			
			<li class="toggle-section" onclick="toggleNested(2, 6)">6. Exploratory data analysis</li>
                        <div class="nested-content" id="nested-2-6">
				<p>The purpose of this phase of the project is to identify <strong>trends</strong>, <strong>patterns</strong>, and validate <strong>hypotheses</strong> through statistical summaries and graphical visualizations.</p>

				<p>To guide the process, several <strong>key questions</strong> were formulated to serve as the foundation for developing and deepening the analysis of the different variables.</p>
				<p><strong><u>KEY QUESTIONS</u></strong></p>
				<p><u>Regarding the borrowers:</u></p>
				<p><ul>
        				<li>Question 1: What are the most common professions of the clients requesting loans?</li>
				</ul></p>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="Imagen3.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 17.5 / 7.5;">
				</div>
				<p>The variable <strong>Employment</strong> is excluded from the analysis because the majority of the data is unknown. This decision stems from the data quality process, where missing values and jobs with a frequency lower than 0.5% were imputed into this "unknown" category.</p>

				<p><ul>
        				<li>Question 2: How does the score assigned by the bank to the client affect the debt status?</li>
				</ul></p>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="Imagen4.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 14.5 / 7.7;">
				</div>
				<p>The <strong>rating</strong> made by the bank, where A represents the clients with the best status regarding the risk of default and G represents the worst, proves to be <strong>predictive</strong> when analyzing the average of cases where the debt is marked as unpaid.</p>

				<p><ul>
        				<li>Question 3: How does the usage percentage, relative to the credit granted on a card, affect the debt status?</li>
				</ul></p>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="Imagen5.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 15 / 8;">
				</div>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="Imagen6.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 18 / 9;">
				</div>
				<p>The <strong>credit card usage percentage</strong> variable proves to be <strong>predictive</strong>, as a higher volume of defaults is observed in clients who have a higher usage percentage. Additionally, three groups can be identified: those who do not use any of the available credit, those who use all the credit, and those who are in a medium usage percentage.</p>
				<p>When analyzing the <strong>default rate</strong> for the profile of clients with 0% usage, it stands at <strong>8.9%</strong>, increasing to <strong>14.6%</strong> for clients with 100% usage. Therefore, the existence of such differentiated groups in terms of usage characteristics and default rate makes this variable a potentially very useful predictor.</p>
				<br>
				<p><u>Regarding the Loans:</u></p>
				<p><ul>
        				<li>Question 4: Does the repayment term of the debt affect the client's default rate?</li>
				</ul></p>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen7.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 17 / 8;">
				</div>
				<p>From the exploratory data analysis (EDA), it can be concluded that, in general terms, clients who take longer <strong>(60 months)</strong> to repay their debt have an average of <strong>6 more points</strong> in their <strong>default rate</strong>.</p>

				<p><ul>
        				<li>Question 5: Does the loan purpose affect the client's default rate?</li>
				</ul></p>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen8.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 18 / 8;">
				</div>
				<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen9.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 18 / 8;">
				</div>
				<p>At first glance, <strong>three groups</strong> can be distinguished regarding the relationship between the loan purpose and the average default rate. On one hand, we have those with a default rate <strong>above 12.5%</strong>, those <strong>between 10% and 12.5%</strong>, and those with a default rate <strong>below 10%</strong>.</p>
				<p>However, when focusing on the frequency within our dataframe, we observe that many categories <strong>lack a sufficient statistical base</strong> to extract reliable patterns. Therefore, a good strategy to retain this information is to group them into the "Other" category.</p>
				<p>Since the "Other" category is in a middle range regarding the default rate, the categories assigned to it, both above and below, will not significantly alter its value.</p>
				<p>Thus, the resulting categories will be: <strong>"Debt Consolidation", "Credit Card" ,"Home Improvement" and "Other."</strong></p>



	

			</div>
			<li class="toggle-section" onclick="toggleNested(2, 7)">7. Feature transformation</li>
                        <div class="nested-content" id="nested-2-7">
			<p>In this stage of the project, various feature transformation techniques will be applied to adapt the data to the specific requirements of the algorithms that will be used during the modeling phase.</p>
			<p>For all three models, categorical variables need to be converted into numerical variables. This process will be carried out using <strong>One-Hot Encoding (OHE)</strong> and <strong>Ordinal Encoding (OE)</strong> techniques. Additionally, a <strong>Binarizer</strong> and a <strong>MinMaxScaler</strong> will be applied, as the logistic regression algorithm is particularly sensitive to differences in variable scales. These transformations ensure that all variables are normalized and ready for analysis.</p>
				
	
			<p><strong><u>CREATING TARGETS</u></strong></p>
			<p>After transforming the variables, it is necessary to create the target variables that will later be used to train the models. The following sections outline the process for each model in detail.</p>
			<p><u>Probability of default (PD):</u></p>
			<p>The purpose of this model is to predict the probability that a given client will default on their loan. To achieve this, the variable "Status" is analyzed, with the primary criterion being that a delay of more than 90 days is classified as a default.</p>
			<p>Below, the frequency distribution of the original "Status" variable is presented.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen10.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 18.2 / 7;">
				</div>
			<p>During the Exploratory Data Analysis (EDA), it was observed that in certain statuses, the bank had recovered part of the capital, indicating that a default had occurred. Based on this analysis, the statuses that indicate default are: <strong>'Charged Off'</strong>, <strong>'Does not meet the credit policy. Status: Charged Off'</strong>, and <strong>'Default'</strong>.</p>
			<p>On the other hand, the statuses that do not indicate default are: <strong>'Current'</strong>, <strong>'Late (31-120 days)'</strong>, <strong>'Late (16-30 days)'</strong>, <strong>'In Grace Period'</strong>, and <strong>'Does not meet the credit policy. Status: Fully Paid'</strong>.</p>
			
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen11.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 15 / 7.6;">
				</div>
			<p>Finally, the </strong>Target PD</strong> variable is defined as a </strong>binary variable (0-1)</strong>, where 0 represents the cases where no default occurs, and 1 represents the cases where a default does happen.</p>

			<p><u>Loss given default (LGD):</u></p>
			<p>The objective of this model is to predict the percentage of the loan that a given borrower has not yet repaid when a default occurs. The target for this model can thus be defined as:</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen12.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 11,3 / 1.8;">
				</div>
			<p><u>Exposure at default (EAD):</u></p>
			<p>The objective of this model is to predict the percentage of the principal that will not be possible to recover from a loan that has been defaulted on. Therefore, the target for this model will be defined as:</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen13.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 6.3 / 1.8;">
				</div>

			</div>




			<li class="toggle-section" onclick="toggleNested(2, 8)">8. Modelling</li>
                        <div class="nested-content" id="nested-2-8">
			<p>At this stage of the project, the primary focus is on the selection and <strong>optimization of predictive models</strong>, along with the search for the best combination of hyperparameters. The goal is to <strong>maximize evaluation metrics</strong> that reflect high predictive power, thereby ensuring robust performance of the selected algorithms.</p>
			<p><strong><u>Probability of default model (PD)</u></strong></p>
			<p>As previously explained, a logistic regression algorithm was chosen for the probability of default (PD) model. This type of model is particularly sensitive to correlations between variables. Therefore, a correlation analysis was conducted, and the results are presented below.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="Imagen14.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 6.5 / 8.2;">
				</div>
			<p>In this analysis, a strong correlation was identified between the variables <strong>tipo_interes</strong> and <strong>rating</strong>, as well as between <strong>importe_cuota</strong> and <strong>principal</strong>. These four variables stand out as the ones with the strongest correlations within the dataset. On the other hand, most of the other correlations detected originate from One-Hot Encoding (OHE) processes. At first glance, it does not seem advisable to remove these OHE-derived variables, as doing so would result in the loss of part of the information contained in the original variables from which they are derived.</p>

			<p>The following analysis focuses exclusively on evaluating the four variables with the highest correlation (<strong>tipo_interes</strong>, <strong>rating</strong>, <strong>importe_cuota</strong>, and <strong>principal</strong>) and determining whether their removal could improve the model's performance. To achieve this, the importance of each variable was analyzed in terms of the predictive power they contribute to the model. As shown in the figure below, these correlated variables also exhibit high predictive power. After comparing the model's evaluation metrics both with and without these variables, it is observed that the predictive performance of the model remains consistent. Therefore, the decision is made to <strong>retain these variables</strong> in the model while acknowledging the presence of these correlations.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen15.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 18.1 / 10.1;">
			
				</div>
			
			<p>To ensure that the results obtained are robust and not influenced by sample bias, cross-validation was performed. The results confirm that there is no overfitting and that the model is stable.</p>
			<p>After performing a GridSearch with different hyperparameter combinations, the configuration that showed the best predictive power was: LogisticRegression(C=1, n_jobs=-1, penalty='l1', solver='saga'). This combination highlights the use of L1 regularization (lasso), which helps to implicitly select features by reducing the less relevant coefficients to zero, and the saga solver, which is suitable for large datasets and compatible with L1 regularization. With this configuration, an AUC of 0.7 was achieved.</p>
			<p>Considering this optimal hyperparameter combination, along with previous tests on the removal of correlated variables, it can be concluded that the information contained in the data defines the limit of the model. Although a model with good predictive capacity was achieved, it does not reach excellent performance.</p>
			<p>To significantly improve the predictive capacity, it would be necessary to incorporate new variables that provide additional and relevant information to the model, as the current features seem to have reached their maximum explanatory potential.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen16.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 14.9 / 9.4;">
			
				</div>
			<p><strong><u>Exposure at default model (EAD)</u></strong></p>
			<p>At this stage, different combinations of algorithms (Ridge, Lasso, LightGBM) and hyperparameters have been tested to identify those with the best performance. It has been found that the <strong>LightGBM</strong> architecture performs the best, achieving a Mean Absolute Error (<strong>MAE</strong>) of <strong>0.23</strong>.</p>
			<p>In the following image, we can see the correlation between the predicted values and the actual values.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen17.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 6.1 / 2.8;">
			
				</div>
			<p>As can be seen, the error made by the model in predicting the Exposure at Default (EAD) when default occurs is high. Nevertheless, it should be noted that errors in this type of risk acquisition models are generally significantly higher than those in behavioural models, marketing, customer management, etc., as much less customer information is available when running the model.</p>
			<p>In the same vein, it should also be noted that both defaulting and non-defaulting borrowers are being modelled, as this information is not available for a new customer. Therefore, on many occasions the model will be trying to predict the exposure at default of borrowers who are unlikely to default, which also explains the level of errors obtained in the modelling.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen18.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 15.1 / 8.9;">
			
				</div>

			<p>It can be seen that in reality (ead_true) three large groups can be distinguished, a majority group of borrowers whose exposure to default is zero, a second group with intermediate exposures (0.25-0.75), and a last group where all those borrowers with greater exposure to default are concentrated.</p>
			<p>Model‚Äôs predictions tend towards intermediate default exposures, which leads to larger errors in predicting those borrowers with very low or very high actual default exposures.</p>
			<ul>
  				<li>For customers who will actually have very limited exposures at default, the model predicts that they will have some degree of exposure to default, which will lead to somewhat higher fees/interest being charged than they would be entitled to.</li>
			</ul>
			<ul>
  				<li>For customers with high actual default exposures: the model will tend to predict lower than actual default exposures, so that lower fees/interest will be applied than would be the case.</li>
			</ul>

			<p>However, at an aggregate level from a business point of view the performance of the model is quite acceptable, as it will be covering the part of fees/interest not collected from borrowers who end up having high exposure defaults with the additional surcharges/interest charged to those customers who eventually did not have defaults, thus covering the aggregate risk of the client portfolio.</p>


			<p><strong><u>Loss given default model (LGD)</u></strong></p>
			<p>Similar to the process carried out in the LGD model, different parameterizations of Ridge, Lasso, and LightGBM algorithms have been tested. Once again, the <strong>LightGBM</strong> architecture provides the best results, achieving a Mean Absolute Error (MAE) of 0.36.</p>
			<p>The error of this model is relatively high, which is explained for the same reasons exposed previously (as it is also a risk acquisition model with limited features available to make predictions).</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen19.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 10.7 / 5;">
			
				</div>
			<p>It can be seen that in reality (lgd_true) two large groups can be distinguished: a group of loans in which no amount is recovered, either because the borrower has not defaulted or because the borrower has defaulted but the bank has not been able to recover any amount; and a second group of loans in which it has been possible to recover the full amount, either because the borrower has amortised the entire loan or because it has been possible to recover the full amount a defaulted loan.</p>
			<p>Model‚Äôs predictions tend towards intermediate loss levels, which leads to larger errors in predicting fully recovered or lost loans.</p>
			<div style="text-align: center;margin-top: 20px;">
    					<img src="imagen20.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 15 / 9.2;">
			
				</div>
			<p>However, as presented for LGD model, the performance of the EAD model is acceptable at an aggregate level from a business point of view, as it will be covering the lost amount of loans in which the amount borrowed has been completely lost by predicting to most customers a loss level of between 25% and 75% even those who ultimately fully paid their loans, thus covering the aggregate risk of the client portfolio.</p>


			<p><strong><u>Final Expected Loss model (EL)</u></strong></p>
			</p>Once the probability of default, exposure at default and loss given default models have been developed, the expected loss (EL) for each new loan application is obtained by simply combining the predictions of these models and the principal amount of the loan as discussed in the methodology section.</p>
			<div style="text-align: center;">
    					<img src="imagen1.png" alt="Project Design Image" style="width: 40%; height: auto; aspect-ratio: 11.5 / 1;">
				</div>

			


			</div>
			<li class="toggle-section" onclick="toggleNested(2, 9)">9. Web app</li>
                        <div class="nested-content" id="nested-2-9">
			<p>In order to get the most value out of the developed machine learning models, it is important to seamlessly deploy it into production so employees can start using them to make practical decisions.</p>
			<p>To this end, a prototype web application has been designed. This web app collects, on the one hand, the internal data that the company has for each client and on the other hand, the information provided by the borrower itself through a loan application.</p>
			<button class="launch-button" onclick="window.location.href='https://appriskscoring-ehuke5ze63jgdfmnspqoqx.streamlit.app/';">
    				Launch Credit Risk Analyzer Web App!
			</button>
			<style>
				launch-button {
        				background-color: #d3d3d3; /* Fondo gris */
        				color: black; /* Color del texto */
        				padding: 20px 40px;
        				font-size: 16px;
        				border: none;
        				border-radius: 5px;
        				cursor: pointer;
        				text-align: center;
        				display: inline-block;
        				width: auto; /* El bot√≥n se ajustar√° al ancho del texto */
        				opacity: 0;
        				animation: fadeIn 1s forwards; /* Efecto de aparici√≥n */
					margin: 0 auto; /* Centrado horizontal */
        				display: block; /* Asegurarse de que el bot√≥n sea un bloque para centrado */
        				transition: background-color 0.3s ease; /* Suavizar el efecto hover */
    
    				}

    				.launch-button:hover {
        				background-color: #b0b0b0; /* Efecto hover para el bot√≥n */
    				}

    				@keyframes fadeIn {
        				0% {
            					opacity: 0;
        				}
        				100% {
            					opacity: 1;
        				}
    				}
			</style>

			</div>
                    </ul>
                </div>
            </div>

            <div class="project-column">
    		<img src="resort.jpg" alt="Hotel management dashboard">
    		<a href="https://public.tableau.com/app/profile/.lvaro.gamundi/viz/Hotelcase/Dashboard1?publish=yes" target="_blank">HOTEL MANAGEMENT DASHBOARD</a>
    		<a href="javascript:void(0);" class="more-info-btn" id="more-info-btn-2" onclick="toggleMoreInfo(2)">More Info</a>
    		<div class="more-info-content" id="more-info-2">
        		<h3>Table of Contents:</h3>
        		<ul>
            			<li class="toggle-section" onclick="toggleNested(10, 10)">1. Introduction</li>
            			<div class="nested-content" id="nested-10-10">
                			<p>The client is an <strong>international hotel group</strong> that has requested the creation of a <strong>dashboard</strong> to integrate all relevant information from their hotels. The goal is to provide a centralized visualization of the data, allowing real-time access to information and facilitating more efficient decision-making.</p>

            			</div>
            
            			<li class="toggle-section" onclick="toggleNested(11, 11)">2. Objectives</li>
            			<div class="nested-content" id="nested-11-11">
                			<p>Create the dashboard according to the specifications provided by the client, including the required KPIs, visualizations, time frame, etc.</p>

            			</div>
				
				<li class="toggle-section" onclick="toggleNested(12, 12)">3. Project designe</li>
            			<div class="nested-content" id="nested-12-12">
					<p>For the execution of this type of project, a <strong>clear methodology</strong> is followed, which helps achieve the objectives more efficiently.</p>
                			<div style="text-align: center;margin-top: 20px;">
    							<img src="imagen21.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 12.4 / 4;">
					</div>
					<p><strong><u>Taking requirements</u></strong></p>
					<p>In this phase, the objective is to have an initial contact with the client to understand the fundamentals of the business and their requirements. It is essential to determine what they expect from the dashboard's functionality, which KPIs they want to obtain, what visualizations they need, and the timeframes in which they want to measure them, among other key aspects.</p>
					<p>The following is a summary of the questions posed to the management team and their corresponding answers.</p>
					<p><strong>What business objectives do you want to achieve by developing this dashboard?</strong></p>
					<ul>
  						<li>To have in a single report the main data necessary for hotel management.</li>
  						<li>With current date and also with a view of developments in the last few months.</li>
  						<li>Achieve unifying all data from the multinational's hotels worldwide under the same visualization format.</li>
					</ul>
					<p><strong>What data specifically would you like to visualize?</strong></p>
					<ul>
  						<li>Revenue.</li>
  						<li>Total reservations.</li>
  						<li>Occupancy rate.</li>
						<li>Average Daily Rate (ADR), which is total room revenue divided by the number of sold rooms.</li>
  						<li>RevPAR (Revenue Per Available Room), which is total room revenue divided by the number of available rooms.</li>
  						<li>Cancelation rate.</li>
	
					</ul>
					<p><strong>Through which dimensions would you like to see such metrics?</strong></p>
					<ul>
  						<li>All the previous metrics throughout the entire time window of the dataset, with the ability to select the time interval to query, at a daily granularity.</li>
  						<li>To know the evolution of the occupancy rate over the last 6 months and its seasonality, at a monthly granularity.</li>
  						<li>Ability to analyze all the previous metrics by country.</li>
					</ul>
					<p><strong><u>Data sources</u></strong></p>
					<p>The next stage of the project is to identify which data sources are available, where they are located, what information is available for each one, etc. In this case, the data provided by the company is contained in a single comma-separated values text file called ‚Äòhotels.csv‚Äô.</p>
					<p>This file directly contains the information of two of the metrics required by the client: the number of bookings and the ADR.</p>

					<p><strong><u>Required calculations and components</u></strong></p>
					<p>In this step the requirements demanded by the customer not directly covered by the information in the data sources are identified and it is determined how to calculate them through the existing information.</p>
					<div style="text-align: center;margin-top: 20px;">
    							<img src="imagen22.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 20.2 / 4;">
					</div>
					<p>Next, the KPIs and visualizations are created according to the requested requirements.</p>
					<div style="text-align: center;margin-top: 20px;">
    							<img src="imagen23.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 21.1 / 4;">
            				</div>
					<p><strong><u>Dashboard designe</u></strong></p>
					<p>Finally, the dashboard design is addressed: its size and structure, its adaptation to different devices (pc, mobile, tablet...), dashboard level filters are set, etc.</p>
					<div style="text-align: center;margin-top: 20px;">
    							<img src="imagen24.png" alt="Project Design Image" style="width: 80%; height: auto; aspect-ratio: 20.1 / 10.9;">
					</div>
        		</ul>
    		</div>
            </div>


	    <div class="project-column">
                <img src="imagen_abandono.jpg" alt="Employee Retention Project">
                <a href="https://employee-retention-analyzer-3djcxzp58ahwzwajrmjhy6.streamlit.app/" target="_blank">EMPLOYEE RETENTION ANALYZER</a>
                <a href="javascript:void(0);" class="more-info-btn" id="more-info-btn-3" onclick="toggleMoreInfo(3)">More Info</a>
                <div class="more-info-content" id="more-info-3">
                    <h3>Project Description:</h3>
                    <p>This project leverages a dataset of employees from a Spanish company to develop an application powered by a Machine Learning model. Its purpose is to predict the probability of an employee leaving, enabling the company to take preventive measures and reduce the economic losses associated with employee turnover.</p>

                    <h3>Technical Details:</h3>
                    <ul>
                        <li>The project addresses a supervised Machine Learning classification problem.</li>
                        <li>Grid Search was used to compare multiple algorithms with different hyperparameter configurations.</li>
                        <li>The best-performing model was a Random Forest, achieving an AUC metric of 79%, standing out for its predictive capability.</li>
                    </ul>

                    <h3>Challenge:</h3>
                    <ul>
                        <li><strong>Data Cleaning:</strong> Addressing data quality issues, ensuring consistency, and handling missing or corrupted data to ensure the models are trained on reliable data.</li>
                        <li><strong>Exploratory Data Analysis (EDA):</strong> Analyzing the dataset to understand the underlying patterns, distributions, and relationships between features, which helped in selecting the right models and preprocessing techniques.</li>
                        <li><strong>Integration and Deployment:</strong> Integrating the model into a production environment and setting up pipelines for automated data processing and model deployment.</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
        const text = "In this space, I showcase the cases I've worked on, focusing on the creation of Machine Learning models, Deep Learning models, and Data Analysis. Explore my projects to learn more about my approach and skills."; 
        let index = 0;

        function typeWriter() {
            if (index < text.length) {
                document.getElementById('typing-text').innerHTML += text.charAt(index);
                index++;
                setTimeout(typeWriter, 35);
            }
        }

        window.onload = typeWriter;

        function toggleMoreInfo(id) {
            const infoContent = document.getElementById(`more-info-${id}`);
            const moreInfoBtn = document.getElementById(`more-info-btn-${id}`);
            if (infoContent.style.display === "block") {
                infoContent.style.display = "none";
                moreInfoBtn.textContent = "More Info";
            } else {
                infoContent.style.display = "block";
                moreInfoBtn.textContent = "Less Info";
            }
        }

        function toggleNested(projectId, sectionId) {
            const nestedContent = document.getElementById(`nested-${projectId}-${sectionId}`);
            nestedContent.style.display = nestedContent.style.display === "block" ? "none" : "block";
        }
    </script>
</body>
</html>




